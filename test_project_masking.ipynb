{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    labels\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
       "1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
       "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
       "3  High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
       "4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"bbc_text_cls.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['business', 'entertainment', 'politics', 'sport', 'tech'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = 'business'\n",
    "texts = df[df['labels'] == label][['text']]\n",
    "isinstance(texts, pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split doc function\n",
    "                  \n",
    "def split_doc(doc):\n",
    "    \n",
    "    return re.findall(r'\\b\\w+\\b', doc)\n",
    "\n",
    "#'\\b\\w+\\b'\n",
    "#The first \\b asserts that we're at a word boundary.\n",
    "#The \\w+ then matches one or more word characters.\n",
    "#The final \\b asserts that we are again at a word boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Ad, sales, boost, Time, Warner, profit, Quart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Dollar, gains, on, Greenspan, speech, The, do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Yukos, unit, buyer, faces, loan, claim, The, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[High, fuel, prices, hit, BA, s, profits, Brit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Pernod, takeover, talk, lifts, Domecq, Shares...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text\n",
       "0  [Ad, sales, boost, Time, Warner, profit, Quart...\n",
       "1  [Dollar, gains, on, Greenspan, speech, The, do...\n",
       "2  [Yukos, unit, buyer, faces, loan, claim, The, ...\n",
       "3  [High, fuel, prices, hit, BA, s, profits, Brit...\n",
       "4  [Pernod, takeover, talk, lifts, Domecq, Shares..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_text'] = df['text'].apply(split_doc)\n",
    "label = 'business'\n",
    "texts = df[df['labels'] == label][['cleaned_text']]\n",
    "isinstance(texts, pd.DataFrame)\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(len(texts))\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_text    [Irish, duo, could, block, Man, Utd, bid, Iris...\n",
      "Name: 178, dtype: object\n"
     ]
    }
   ],
   "source": [
    "doc = texts.iloc[i]\n",
    "pprint(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ITERATE OVER A LIST OF LISTS IN A DF COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m vectorizer \u001b[39m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m      4\u001b[0m \u001b[39m# Fit and transform the text data from the DataFrame\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m tfidf_matrix \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(texts[\u001b[39m'\u001b[39;49m\u001b[39mcleaned_text\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      7\u001b[0m \u001b[39m# Get feature names\u001b[39;00m\n\u001b[0;32m      8\u001b[0m feature_names \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mget_feature_names_out()\n",
      "File \u001b[1;32mc:\\Users\\b.ludwicki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2121\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2122\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2123\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2124\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2125\u001b[0m )\n\u001b[1;32m-> 2126\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2128\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\b.ludwicki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\b.ludwicki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1375\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1376\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1377\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1380\u001b[0m             )\n\u001b[0;32m   1381\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1383\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1385\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1386\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\b.ludwicki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1269\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1270\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1271\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1272\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\b.ludwicki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mc:\\Users\\b.ludwicki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[1;32m---> 68\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data from the DataFrame\n",
    "tfidf_matrix = vectorizer.fit_transform(texts['cleaned_text'])\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a dictionary to store each word and its TF-IDF score\n",
    "word_tfidf_dict = {}\n",
    "\n",
    "# Loop through each document (each row in tfidf_matrix)\n",
    "for i in range(tfidf_matrix.shape[0]):\n",
    "    # Get the TF-IDF vector for the i-th document in sparse format\n",
    "    feature_index = tfidf_matrix[i,:].nonzero()[1]\n",
    "    \n",
    "    # Get the corresponding TF-IDF score\n",
    "    tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
    "    \n",
    "    # Update the word_tfidf_dict\n",
    "    for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
    "        word_tfidf_dict[w] = s\n",
    "\n",
    "print(word_tfidf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_assigner(doc):\n",
    "\n",
    "    assigner = {}\n",
    "    for word in word_list:\n",
    "        frequency = word_tfidf_dict.get(word,0)\n",
    "\n",
    "        assigner[word] = frequency\n",
    "\n",
    "    print(assigner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'German': 0, 'jobless': 0.13857474873795, 'rate': 0.16221722159662497, 'at': 0.046649657966246955, 'new': 0.00811857602630632, 'record': 0.013449250440927919, 'More': 0, 'than': 0.0533377515534812, '5': 0, '2': 0, 'million': 0.024796463909582143, 'Germans': 0, 'were': 0.026388157409469492, 'out': 0.027578504660403087, 'of': 0.15971387645462387, 'work': 0.04252804846508902, 'in': 0.11943747226690583, 'February': 0, 'figures': 0.023187666309470556, 'show': 0.09266935669819228, 'The': 0, 'figure': 0.018259844834380957, '216': 0.08078112017899494, 'people': 0.06779507922254037, 'or': 0.07031540991115011, '12': 0.02481726398405992, '6': 0, 'the': 0.250026233163237, 'working': 0.0073988432306718345, 'age': 0.04626780077018615, 'population': 0.031840019021927596, 'is': 0.12016478065486096, 'highest': 0.09072058992086972, 'Europe': 0, 's': 0, 'biggest': 0.014844424101985908, 'economy': 0.031147672953674738, 'since': 0.028033269466798003, '1930s': 0.055435486472234494, 'news': 0.02409402355382457, 'comes': 0.041926181132680235, 'as': 0.054859292443210944, 'head': 0.017727630541088075, 'Germany': 0, 'panel': 0.062385982653254145, 'government': 0.005699489602089406, 'economic': 0.021908070317575745, 'advisers': 0.06907599765170534, 'predicted': 0.04081393224922295, 'growth': 0.014579850372724043, 'would': 0.02598798318168308, 'again': 0.03454056402491988, 'stagnate': 0.09970859590352375, 'Speaking': 0, 'on': 0.04898320821299338, 'TV': 0, 'Bert': 0, 'Ruerup': 0, 'said': 0.005197682243710186, 'earlier': 0.006991832765764248, 'forecast': 0.0539145139814494, '1': 0, '4': 0, 'was': 0.05810523369999857, 'too': 0.04598561283998098, 'optimistic': 0.0835031397232488, 'and': 0.14641412670146545, 'warned': 0.03288102427664432, 'be': 0.055876709848505575, 'just': 0.03598397977648616, '2005': 0.014016589634371858, 'trying': 0.016055138743499026, 'to': 0.20854583411681518, 'tackle': 0.0353070387065212, 'stubbornly': 0.044280844249930076, 'high': 0.01742622442513734, 'levels': 0.00922697380865618, 'joblessness': 0.0823108266547058, 'with': 0.047835224892776175, 'a': 0, 'range': 0.02393495583931798, 'labour': 0.027495766626006373, 'market': 0.02626123773632965, 'reforms': 0.02610305653868794, 'At': 0, 'their': 0.052525730392061265, 'centre': 0.022607855637356, 'Hartz': 0, 'IV': 0, 'programme': 0.01716904186792175, 'introduced': 0.059274316361370934, 'January': 0, 'shake': 0.0352347427060974, 'up': 0.02124198298759579, 'welfare': 0.030081734436199468, 'benefits': 0.02796926563970618, 'push': 0.030326465703152825, 'back': 0.005207994449209252, 'into': 0.027457574902334556, 'even': 0.018973179096571147, 'if': 0.03991227355285262, 'some': 0.06657261534927371, 'jobs': 0.025348906532697724, 'are': 0.06674888116668493, 'heavily': 0.027097291054040965, 'subsidised': 0.10739422533518692, 'latest': 0.04762203263678847, 'unemployment': 0.03850684010850842, 'look': 0.02247721221919709, 'set': 0.015164209848520441, 'increase': 0.02296053769526206, 'pressure': 0.07886029240186584, 'Widely': 0, 'leaked': 0.04109068936186353, 'newspapers': 0.048951622019123994, 'day': 0.058321831423887666, 'advance': 0.041094411304153255, 'they': 0.04441106177710917, 'produced': 0.03805399247094021, 'screaming': 0.05000295655588436, 'headlines': 0.03435305400759229, 'criticising': 0.07195720654234547, 'Chancellor': 0, 'Gerhard': 0, 'Schroeder': 0, 'Social': 0, 'Democrat': 0, 'Green': 0, 'Party': 0, 'administration': 0.032807801693990314, 'Mr': 0, 'had': 0.025113722830746074, 'originally': 0.030612693090341297, 'come': 0.006274782610946162, 'office': 0.01675825113893102, 'promising': 0.030936315287626314, 'halve': 0.044430855527847556, 'Still': 0, 'measures': 0.037016863991355904, 'suggest': 0.03577466522595986, 'picture': 0.02827314826591056, 'not': 0.046491993488590046, 'quite': 0.01725080304232074, 'so': 0.061979330783141884, 'bleak': 0.0820861174047656, 'soaring': 0.049800417663137524, 'official': 0.030336374310211956, 'follows': 0.031480091077086006, 'change': 0.007600477418366796, 'methodology': 0.07479718009617213, 'which': 0.006707517850514264, 'pushed': 0.03165770867154072, 'by': 0.03103192044179693, 'more': 0.09469450829834537, '500': 0.027477466833189836, '000': 0.00562491474840765, 'Adjusted': 0, 'for': 0.07614492703388555, 'seasonal': 0.07358675359106459, 'changes': 0.055720699965551936, 'overall': 0.019555050557895467, '875': 0.09290562288390404, '11': 0.015200954836733591, '7': 0, '0': 0, '3': 0, 'percentage': 0.043887281415253304, 'points': 0.027916491267386948, 'from': 0.027157168342502116, 'previous': 0.0769212701690179, 'month': 0.012960406983086146, 'Using': 0, 'most': 0.021462186931969655, 'internationally': 0.05050215925427768, 'accepted': 0.026045196455654115, 'International': 0, 'Labour': 0, 'Organisation': 0, 'ILO': 0, '97': 0.03818943341751635, 'And': 0, 'based': 0.04463326889605547, 'also': 0.03585428784076537, 'that': 0.09792493541966875, '14': 0.028029845515453174, 'net': 0.07861104884295593, 'created': 0.030972158693899427, 'taking': 0.007113470941822074, 'number': 0.0434920338834132, 'employed': 0.033117836771503394, '38': 0.04919291366719724, '9': 0, 'defines': 0.048865425879569525, 'an': 0.06127430036817245, 'unemployed': 0.053114023234709894, 'person': 0.00939418177241978, 'someone': 0.009657117921845645, 'who': 0.03818180852785973, 'four': 0.006143778906384508, 'weeks': 0.007809916257359064, 'actively': 0.0732607152259579, 'looked': 0.04089434291009228, 'could': 0.021571408853412813, 'take': 0.02120474183364163, 'immediately': 0.052735216620552125}\n"
     ]
    }
   ],
   "source": [
    "dict = freq_assigner(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
